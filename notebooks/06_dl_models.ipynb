{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Deep Learning Models**\n",
        "   Goal: Capture complex temporal patterns\n",
        "## Models\n",
        " - LSTM\n",
        " - GRU\n",
        " - Temporal CNN\n",
        " - Seq2Seq\n",
        " - Transformer (advanced)\n",
        "\n",
        "## Techniques\n",
        " - Sliding windows\n",
        " - Multivariate sequences\n",
        " - Early stopping\n",
        " - GPU acceleration (if available)\n",
        "\n",
        "  Output: Best DL model\n"
      ],
      "metadata": {
        "id": "8uE9F8hrSSVG"
      },
      "id": "8uE9F8hrSSVG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DL Foundations & Data Preparation\n",
        " - Load feature-engineered dataset\n",
        " - Select DL-safe features\n",
        " - Normalize numeric features (fit on train only)\n",
        " - Encode categoricals (embeddings-ready)\n",
        " - Create sliding windows\n",
        " - Define sequence tensors\n",
        "### concepts\n",
        " - Multivariate time series\n",
        " - Sliding window formulation\n",
        " - Sequence-to-one forecasting\n",
        "\n",
        "### Output\n",
        " - X_train_seq, y_train_seq\n",
        " - X_valid_seq, y_valid_seq\n",
        " - scaler.pkl\n",
        " - feature_index.json## Load Feature-Engineered Dataset"
      ],
      "metadata": {
        "id": "m5K23xzuT3Ur"
      },
      "id": "m5K23xzuT3Ur"
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone GitHub Repository\n",
        "!git clone https://github.com/sabin74/Enterprise-Intelligent-Demand-Forecasting-Decision-Optimization-Platform.git"
      ],
      "metadata": {
        "id": "_mmtDjH-R2v3"
      },
      "id": "_mmtDjH-R2v3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "0BQq1rwgUI1R"
      },
      "id": "0BQq1rwgUI1R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds for reproducibility\n",
        "import tensorflow as tf\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "np5wUYCD-ZVR"
      },
      "id": "np5wUYCD-ZVR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Project Root\n",
        "os.chdir('/content/Enterprise-Intelligent-Demand-Forecasting-Decision-Optimization-Platform')\n",
        "print(\"Current Directory: \", os.getcwd())"
      ],
      "metadata": {
        "id": "u7HMeartUebL"
      },
      "id": "u7HMeartUebL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Feature Engineered Data\n",
        "df = pd.read_parquet('data/features/train_features.parquet')\n",
        "\n",
        "df['data'] = pd.to_datetime(df['date'])\n",
        "df = df.sort_values(['store_nbr', 'family', 'date']).reset_index(drop=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "vOTKzhSGUwP5"
      },
      "id": "vOTKzhSGUwP5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Validatoin Split\n",
        "split_date = '2017-01-01'\n",
        "train_df = df[df['data'] < split_date].copy()\n",
        "valid_df = df[df['data'] >= split_date].copy()\n",
        "\n",
        "print(df.shape, train_df.shape, valid_df.shape)"
      ],
      "metadata": {
        "id": "Vb9i6LkoVNdX"
      },
      "id": "Vb9i6LkoVNdX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Selection\n",
        "TARGET = \"sales_log\"\n",
        "\n",
        "NUMERIC_FEATURES = [\n",
        "    \"onpromotion\",\n",
        "    \"cluster\",\n",
        "    \"dcoilwtico\",\n",
        "    \"is_holiday\",\n",
        "    \"is_workday\",\n",
        "    \"earthquake\",\n",
        "    \"is_payday\",\n",
        "    \"week_of_year\",\n",
        "    \"is_weekend\",\n",
        "    \"is_month_end\",\n",
        "\n",
        "    \"sales_lag_1\",\n",
        "    \"sales_lag_7\",\n",
        "    \"sales_lag_14\",\n",
        "    \"sales_lag_28\",\n",
        "\n",
        "    \"promo_lag_1\",\n",
        "    \"promo_lag_7\",\n",
        "\n",
        "    \"oil_lag_7\",\n",
        "    \"oil_lag_14\",\n",
        "    \"oil_lag_28\",\n",
        "\n",
        "    \"sales_roll_mean_7\",\n",
        "    \"sales_roll_mean_14\",\n",
        "    \"sales_roll_mean_28\",\n",
        "\n",
        "    \"sales_roll_std_7\",\n",
        "    \"sales_roll_std_14\",\n",
        "    \"sales_roll_std_28\",\n",
        "\n",
        "    \"promo_roll_sum_7\",\n",
        "    \"promo_roll_sum_14\",\n",
        "    \"promo_roll_sum_28\",\n",
        "\n",
        "    \"promo_flag\",\n",
        "    \"promo_freq_7\",\n",
        "    \"promo_freq_14\",\n",
        "    \"promo_freq_28\",\n",
        "\n",
        "    \"is_national_holiday\",\n",
        "    \"is_regional_holiday\",\n",
        "    \"is_local_holiday\",\n",
        "    \"is_bridge\",\n",
        "    \"is_comp_workday\",\n",
        "    \"is_pre_holiday\",\n",
        "    \"is_post_holiday\",\n",
        "\n",
        "    \"family_freq\",\n",
        "    \"store_freq\",\n",
        "    \"city_freq\",\n",
        "    \"state_freq\",\n",
        "]\n",
        "\n",
        "\n",
        "CATEGORICAL_FEATURES = [\n",
        "    \"store_nbr\",\n",
        "    \"family\",\n",
        "    \"city\",\n",
        "    \"state\",\n",
        "    \"store_type\",\n",
        "    \"holiday_type\",\n",
        "    \"locale\",\n",
        "]"
      ],
      "metadata": {
        "id": "KZWa3sRPWJfV"
      },
      "id": "KZWa3sRPWJfV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode Categoricals (Integer IDs for Embeddings)\n",
        "# Build Category\n",
        "category_maps = {}\n",
        "\n",
        "for col in CATEGORICAL_FEATURES:\n",
        "    category_maps[col] = {\n",
        "        category: idx + 1\n",
        "        for idx, category in enumerate(train_df[col].astype(str).unique())\n",
        "    }"
      ],
      "metadata": {
        "id": "30uuu5I6XhHK"
      },
      "id": "30uuu5I6XhHK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Encoding\n",
        "def encode_categoreis(df, category_maps):\n",
        "  df = df.copy()\n",
        "  for col, mapping in category_maps.items():\n",
        "    df[col] = df[col].astype(str).map(mapping).fillna(-1).astype(int)\n",
        "  return df\n",
        "\n",
        "train_df = encode_categoreis(train_df, category_maps)\n",
        "valid_df = encode_categoreis(valid_df, category_maps)"
      ],
      "metadata": {
        "id": "QOu0ALSZY1Dg"
      },
      "id": "QOu0ALSZY1Dg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize Numeric Features\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "train_df[NUMERIC_FEATURES] = scaler.fit_transform(train_df[NUMERIC_FEATURES])\n",
        "valid_df[NUMERIC_FEATURES] = scaler.transform(valid_df[NUMERIC_FEATURES])"
      ],
      "metadata": {
        "id": "s2U5B9xZZdpO"
      },
      "id": "s2U5B9xZZdpO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Memory Optimization (reduce memory usage)\n",
        "def reduce_mem_usage(df, ):\n",
        "  for col in df.columns:\n",
        "    if df[col].dtype == \"float64\":\n",
        "      df[col] = df[col].astype(\"float32\")\n",
        "    elif df[col].dtype == \"int64\":\n",
        "      df[col] = df[col].astype(\"int32\")\n",
        "  return df\n",
        "\n",
        "train_df = reduce_mem_usage(train_df)\n",
        "test_df = reduce_mem_usage(valid_df)\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "BcyjBF5hxLDL"
      },
      "id": "BcyjBF5hxLDL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Final Feature Order\n",
        "DL_FEATURES = NUMERIC_FEATURES + CATEGORICAL_FEATURES\n",
        "len(DL_FEATURES)\n",
        "\n",
        "train_df = train_df[DL_FEATURES + [TARGET]]\n",
        "valid_df = valid_df[DL_FEATURES + [TARGET]]"
      ],
      "metadata": {
        "id": "GZ1_PChNbjo7"
      },
      "id": "GZ1_PChNbjo7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sliding Window Generator (Reusable Engine)\n",
        "\n",
        "All DL models will reuse this logic.\n",
        "\n",
        "### Build window generator:\n",
        " - window_size = 28\n",
        " - horizon = 1\n",
        " - Ensure time continuity per store-family\n",
        " - Batch-safe generator (tf / torch compatible)\n",
        "\n",
        "### Output\n",
        " -  Sliding window function\n",
        " -  Memory-efficient batching\n",
        " -  Tested on small subset"
      ],
      "metadata": {
        "id": "Qcl0BmxZjUEY"
      },
      "id": "Qcl0BmxZjUEY"
    },
    {
      "cell_type": "code",
      "source": [
        "# Sliding Window Function\n",
        "def sliding_window_generator(\n",
        "    df,\n",
        "    window_size=28,\n",
        "    horizon=1,\n",
        "    feature_cols=None,\n",
        "    target_col=\"sales_log\",\n",
        "    group_cols=(\"store_nbr\", \"family\"),\n",
        "    batch_size=256,\n",
        "):\n",
        "    \"\"\"\n",
        "    Memory-safe sliding window generator.\n",
        "    Yields batches for DL training.\n",
        "    \"\"\"\n",
        "\n",
        "    X_batch, y_batch = [], []\n",
        "\n",
        "    for _, gdf in df.groupby(list(group_cols)):\n",
        "        gdf = gdf.sort_values(\"date\")\n",
        "\n",
        "        X_values = gdf[feature_cols].values.astype(\"float32\")\n",
        "        y_values = gdf[target_col].values.astype(\"float32\")\n",
        "\n",
        "        total_len = len(gdf)\n",
        "        if total_len < window_size + horizon:\n",
        "            continue\n",
        "\n",
        "        for i in range(total_len - window_size - horizon + 1):\n",
        "            X_batch.append(X_values[i : i + window_size])\n",
        "            y_batch.append(y_values[i + window_size + horizon - 1])\n",
        "\n",
        "            if len(X_batch) == batch_size:\n",
        "                yield np.array(X_batch), np.array(y_batch)\n",
        "                X_batch, y_batch = [], []\n",
        "\n",
        "    if X_batch:\n",
        "        yield np.array(X_batch), np.array(y_batch)\n"
      ],
      "metadata": {
        "id": "POhMg8_SpFT5"
      },
      "id": "POhMg8_SpFT5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Train / Validation Generator\n",
        "WINDOW_SIZE = 28\n",
        "HORIZON = 1\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "train_gen = sliding_window_generator(\n",
        "    df=train_df,\n",
        "    window_size=WINDOW_SIZE,\n",
        "    horizon=HORIZON,\n",
        "    feature_cols=DL_FEATURES,\n",
        "    target_col=\"sales_log\",\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "\n",
        "valid_gen = sliding_window_generator(\n",
        "    df=valid_df,\n",
        "    window_size=WINDOW_SIZE,\n",
        "    horizon=HORIZON,\n",
        "    feature_cols=DL_FEATURES,\n",
        "    target_col=\"sales_log\",\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n"
      ],
      "metadata": {
        "id": "M7BoM8oyqDCS"
      },
      "id": "M7BoM8oyqDCS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity Check\n",
        "X_train, y_train = next(train_gen)\n",
        "print(\"Train batch:\", X_train.shape, y_train.shape)\n",
        "\n",
        "X_valid, y_valid = next(valid_gen)\n",
        "print(\"Valid batch:\", X_valid.shape, y_valid.shape)"
      ],
      "metadata": {
        "id": "Olrw7E7gqL3n"
      },
      "id": "Olrw7E7gqL3n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Save DL Scaler\n",
        "import joblib\n",
        "import json\n",
        "\n",
        "joblib.dump(scaler, 'models/scaler.pkl')\n",
        "\n",
        "# Save Feature Map\n",
        "feature_map = {\n",
        "    \"numeric_features\": NUMERIC_FEATURES,\n",
        "    \"categorical_features\": CATEGORICAL_FEATURES,\n",
        "    \"dl_features_order\": DL_FEATURES,\n",
        "    \"category_maps\": category_maps,\n",
        "    \"window_size\": WINDOW_SIZE,\n",
        "}\n",
        "\n",
        "with open(\"models/dl_feature_map.json\", \"w\") as f:\n",
        "    json.dump(feature_map, f, indent=4)\n"
      ],
      "metadata": {
        "id": "_-zviRP1qbF5"
      },
      "id": "_-zviRP1qbF5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1: LSTM (Baseline DL)\n",
        " - Handles long dependencies\n",
        " - Strong baseline\n",
        " - Easy to interpret\n",
        "### What we do\n",
        " - Build LSTM architecture\n",
        " - Sequence â†’ Dense output\n",
        " - Early stopping\n",
        " - GPU detection\n",
        "\n",
        "### Output\n",
        " - LSTM RMSLE\n",
        " - Training curves\n",
        " - Saved model"
      ],
      "metadata": {
        "id": "52WjHUbwulym"
      },
      "id": "52WjHUbwulym"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "ZqdKTF5A1_ef"
      },
      "id": "ZqdKTF5A1_ef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Metrics\n",
        "def rmsle(y_true, y_pred):\n",
        "    y_true = np.maximum(y_true, 0)\n",
        "    y_pred = np.maximum(y_pred, 0)\n",
        "    return np.sqrt(np.mean((y_true - y_pred) ** 2))"
      ],
      "metadata": {
        "id": "SvO1Ou0r_oWw"
      },
      "id": "SvO1Ou0r_oWw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Built LSMT Model - Basseline Architecture\n",
        "num_features = X_train.shape[2]\n",
        "\n",
        "lstm_model = Sequential([\n",
        "    LSTM(units=64, return_sequences=True, input_shape=(WINDOW_SIZE, num_features)),\n",
        "    Dropout(0.2),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='mse')\n",
        "lstm_model.summary()"
      ],
      "metadata": {
        "id": "9Ayoa87z360w"
      },
      "id": "9Ayoa87z360w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Early Stoppings\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True\n",
        ")"
      ],
      "metadata": {
        "id": "N-pZrl495AQy"
      },
      "id": "N-pZrl495AQy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train_model\n",
        "history = lstm_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_train, y_valid),\n",
        "    epochs=30,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "elkNelPt5Br4"
      },
      "id": "elkNelPt5Br4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RMSLE Functoin\n",
        "def rmsle(y_true, y_pred):\n",
        "  y_true = np.expm1(y_true)\n",
        "  y_pred = np.expm1(y_pred)\n",
        "  y_pred = np.maximum(y_pred, 0)\n",
        "  return np.sqrt(np.mean((np.log1p(y_true) - np.log1p(y_pred))**2))\n"
      ],
      "metadata": {
        "id": "OzVgysWe6mWE"
      },
      "id": "OzVgysWe6mWE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict and Evaluate\n",
        "y_valid_pred = lstm_model.predict(X_valid).reshape(-1)\n",
        "\n",
        "lstm_rmsle = rmsle(y_valid, y_valid_pred)\n",
        "print(f\"LSTM RMSLE: {lstm_rmsle:.4f}\")\n"
      ],
      "metadata": {
        "id": "qwYkBECt7RWN"
      },
      "id": "qwYkBECt7RWN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_dataset_from_generator(generator_fn):\n",
        "    return tf.data.Dataset.from_generator(\n",
        "        generator_fn,\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=(None, WINDOW_SIZE, len(DL_FEATURES)), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(None,), dtype=tf.float32),\n",
        "        ),\n",
        "    ).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "G6m9LfUo7x-6"
      },
      "id": "G6m9LfUo7x-6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = tf_dataset_from_generator(lambda: sliding_window_generator(\n",
        "    train_df, WINDOW_SIZE, HORIZON, DL_FEATURES, \"sales_log\", batch_size=256\n",
        "))\n",
        "\n",
        "valid_ds = tf_dataset_from_generator(lambda: sliding_window_generator(\n",
        "    valid_df, WINDOW_SIZE, HORIZON, DL_FEATURES, \"sales_log\", batch_size=256\n",
        "))\n"
      ],
      "metadata": {
        "id": "xB9EpMFG_SGG"
      },
      "id": "xB9EpMFG_SGG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ldKQ6d7dDSnG"
      },
      "id": "ldKQ6d7dDSnG",
      "execution_count": null,
      "outputs": []
    }
  ]
}